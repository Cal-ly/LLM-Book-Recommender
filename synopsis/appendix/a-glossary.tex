\chapter{Glossary of Terms}
\label{appendix:glossary}

This appendix defines key terms used throughout the project, with a focus on Machine Learning, Natural Language Processing, and Information Retrieval. Each entry aims to explain the term in accessible language, suitable for readers with a general computer science background.

\begin{description}
    \item[AI (Artificial Intelligence):] The simulation of human intelligence in machines that can reason, learn, and make decisions.
    \item[Attention Mechanism:] A component of neural networks, especially transformers, that allows the model to focus on relevant parts of the input sequence when making predictions.
    \item[Augmentation (Data):] The process of enriching data with external sources to fill in missing values or increase its descriptive quality.
    \item[BART (Bidirectional and Auto-Regressive Transformer):] A transformer model used for text generation and classification tasks. In this project, it powers zero-shot classification.
    \item[BERT (Bidirectional Encoder Representations from Transformers):] A transformer-based model that generates contextual embeddings for text, capturing semantic meaning. It is foundational for many NLP tasks.
    \item[Category Inference:] Assigning thematic or genre labels to text entries based on their content.
    \item[Classification:] The task of assigning predefined labels to text based on its content. In this project, it is used to infer book genres.
    \item[Cold Start Problem:] A challenge in recommender systems where there is insufficient data about new users or items to make accurate recommendations.
    \item[Content-Based Recommendation:] A technique that recommends items based on their attributes rather than user behavior.
    \item[Cosine Similarity:] A metric that measures the cosine of the angle between two vectors, indicating how similar they are in direction regardless of magnitude.
    \item[Dimensionality Reduction:] Techniques used to reduce the number of features in data while preserving important information, often used before similarity search.
    \item[Embedding:] The transformation of text into a numeric vector that captures its meaning in a high-dimensional space.
    \item[Embedding Function:] A function or model that converts text into a vector representation. In this project, it is implemented using MiniLM.
    \item[Embedding Model:] A neural network that transforms text into fixed-length numeric vectors, capturing semantic meaning. This project uses MiniLM for sentence embeddings.
    \item[Embedding-Based Search:] A search technique that uses vector representations of text to find semantically similar items rather than relying solely on keyword matching.
    \item[FAISS (Facebook AI Similarity Search):] A library that enables efficient vector similarity search, often used for large-scale nearest neighbor search.
    \item[FAISS Index:] A data structure used by FAISS to store and retrieve high-dimensional vectors efficiently. It allows for fast similarity searches.
    \item[Feature Extraction:] The process of transforming raw data into a set of features that can be used for machine learning tasks.
    \item[Filtering:] Reducing a set of results based on certain constraints (e.g., rating threshold, genre tag).
    \item[Fine-Tuning:] The process of taking a pre-trained model and training it further on a specific dataset to adapt it to a particular task.
    \item[GloVe (Global Vectors for Word Representation):] A model that generates word embeddings by analyzing word co-occurrence statistics in a corpus, capturing global semantic relationships.
    \item[Ground Truth:] The actual, real-world values or labels used as a benchmark to evaluate model predictions.
    \item[Hybrid Recommendation:] A system that combines multiple recommendation strategies, such as content-based and collaborative filtering, to improve results.
    \item[Inference (Model):] The process of using a trained model to make predictions on new, unseen data.
    \item[L2 Distance (Euclidean):] A metric that measures the straight-line distance between two vectors in space.
    \item[Local-First Design:] A system architecture that prioritizes local computation and storage, minimizing reliance on external servers.
    \item[MiniLM:] A lightweight sentence embedding model that provides efficient semantic representations with low computational cost.
    \item[MultiNLI (Multi-Genre Natural Language Inference):] A dataset used for training models on natural language inference tasks, where the goal is to determine if a hypothesis logically follows from a premise.
    \item[Natural Language Generation (NLG):] A subfield of NLP that focuses on generating human-like text from structured data or other inputs.
    \item[Natural Language Inference (NLI):] A task in NLP where the goal is to determine if a hypothesis logically follows from a premise, often used for zero-shot classification.
    \item[Natural Language Processing (NLP):] A field of AI focused on the interaction between computers and human language, enabling machines to understand, interpret, and generate text.
    \item[Natural Language Understanding (NLU):] A subfield of NLP that focuses on enabling machines to understand the meaning and intent behind human language.
    \item[Overfitting:] When a model learns the training data too well, including its noise, resulting in poor generalization to new data.
    \item[Pearson Correlation:] A measure of the linear correlation between two variables, indicating how well one can predict the other.
    \item[Pre-trained Model:] A model that has been previously trained on a large dataset and can be fine-tuned for specific tasks, reducing the need for extensive training from scratch.
    \item[Precision (Information Retrieval):] The proportion of retrieved items that are actually relevant.
    \item[Query:] A user-inputted string used to search or retrieve information from a system.
    \item[Ranking:] The process of ordering items (e.g., search results or recommendations) based on their relevance or predicted interest to the user.
    \item[Recall (Information Retrieval):] The proportion of relevant items that are successfully retrieved by a search or recommendation system.
    \item[Recommender System:] A system that suggests items of interest to users based on various algorithms.
    \item[Semantic Search:] A search technique that considers the meaning of the query rather than just keyword matching.
    \item[Sentence Embedding:] A vector representation of a sentence that captures its semantic meaning.
    \item[Similarity Metric:] A function that quantifies how alike two objects (such as vectors or texts) are, commonly used in search and recommendation.
    \item[Spearman Correlation:] A non-parametric measure of rank correlation that assesses how well the relationship between two variables can be described by a monotonic function.
    \item[Stop Words:] Common words (such as "the", "is", "and") that are often removed during text preprocessing because they carry little semantic meaning.
    \item[Streamlit:] A lightweight Python framework for building interactive web interfaces for machine learning and data science applications.
    \item[Token:] A basic unit of text, such as a word or subword, that is processed by NLP models. Tokenization is the process of converting text into these units. 
    \item[Tokenization:] The process of breaking down text into smaller units (tokens), such as words or subwords, for further processing.
    \item[Transformer:] A neural network architecture designed to handle sequential data, commonly used in language models.
    \item[User Profile:] A representation of a user's preferences, interests, or behavior, used to personalize recommendations.
    \item[Vector Index:] A data structure that stores embedded vectors for fast similarity retrieval.
    \item[Word Embedding:] A representation of words in a continuous vector space, capturing semantic relationships. Examples include Word2Vec and GloVe.
    \item[Word2Vec:] A model that generates word embeddings by predicting surrounding words in a sentence, capturing semantic relationships based on context.
    \item[Zero-Shot Classification:] A method where a model assigns labels it has never seen during training, using semantic understanding.
\end{description}
