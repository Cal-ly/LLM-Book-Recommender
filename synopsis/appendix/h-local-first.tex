\chapter{Local-First System Design Principles}
\label{appendix:local-first}

This appendix outlines the principles behind local-first architecture, as applied to this project’s end-to-end recommendation system. It explains how privacy, autonomy, and offline usability influenced the choice of tools and methods.

\section{What is Local-First Software?}
Local-first software prioritizes performing all computation and storage on the user’s own device. Network access is optional, not essential. Key motivations include:

\begin{itemize}
    \item \textbf{Privacy:} No personal data is sent to remote servers.
    \item \textbf{Reliability:} The system functions without internet access.
    \item \textbf{Ownership:} The user retains full control over their data and execution.
\end{itemize}

This paradigm aligns well with AI applications where model inference is possible without cloud dependencies.

\section{Application to This Project}
Every step of the pipeline was designed to run locally:

\begin{itemize}
    \item \textbf{Model inference:} All embedding and classification done using CPU-friendly models (MiniLM, BART-MNLI).
    \item \textbf{Storage:} All data and embeddings stored in local CSV and FAISS index files.
    \item \textbf{Interface:} The Streamlit app runs entirely within a browser on localhost.
\end{itemize}

\section{Architectural Choices}
The system was built around modular, loosely coupled components:

\begin{enumerate}
    \item Python scripts for data ingestion, augmentation, and embedding
    \item FAISS index for efficient vector search
    \item Streamlit frontend for interaction
\end{enumerate}

Each step can be re-run or updated independently, supporting rapid experimentation and debugging.

\section{Trade-Offs and Limitations}
\begin{itemize}
    \item \textbf{No personalization:} Without user tracking or history, recommendations are purely content-based.
    \item \textbf{Compute constraints:} Model size is limited by local CPU/RAM availability.
    \item \textbf{No online learning:} The system does not evolve based on user behavior unless manually re-trained.
\end{itemize}

These limitations are acceptable within the project’s scope and enable greater user autonomy.

\section{Offline AI: Why It Matters}
Running AI locally is increasingly viable due to efficient models and improved hardware. Benefits include:

\begin{itemize}
    \item Resilience in low-connectivity environments (e.g., education, fieldwork)
    \item Cost reduction (no cloud inference or hosting fees)
    \item Alignment with GDPR and user-rights perspectives
\end{itemize}

\section{Comparison with Cloud-Based Systems}
{\small
\begin{tabular}{@{}p{4.5cm} p{4.5cm} p{4.5cm}@{}}
\toprule
\textbf{Aspect} & \textbf{Local-First} & \textbf{Cloud-Based} \\
\midrule
Latency & Sub-second (no network delay) & Depends on API round-trips \\
Privacy & No data leaves device & Data often logged/transferred \\
Scalability & Limited to local hardware & Scales elastically with demand \\
Update Mechanism & Manual or scripted & Centralized and automatic \\
Dependency & Self-contained & Relies on service availability \\
\bottomrule
\end{tabular}
}

\section{Conclusion}
Local-first design makes AI more accessible, ethical, and sustainable. By avoiding network reliance, the system empowers users to explore semantic search capabilities on their own terms, without compromising performance or flexibility.

