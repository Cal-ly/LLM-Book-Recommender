\chapter{Sentence Embeddings}
\label{appendix:sentence-embeddings}

Sentence embeddings are dense vector representations of entire sentences, designed to capture semantic meaning. Unlike traditional bag-of-words models, embeddings preserve word order and contextual meaning, allowing for nuanced comparisons between texts.

\section*{Intuition}
Where classic keyword-based systems rely on surface matches, sentence embeddings map similar meanings to nearby points in vector space. For instance:

\begin{quote}
\texttt{"A tale of survival in space"}
\end{quote}
\begin{quote}
\texttt{"Story about astronauts facing danger beyond Earth"}
\end{quote}

Though these phrases share few keywords, their embeddings are close in high-dimensional space.

\section*{Transformer-Based Embeddings}
Transformer models such as BERT, RoBERTa, and MiniLM process text using self-attention mechanisms. They generate contextual embeddings â€” meaning the word \texttt{bank} in "river bank" and "investment bank" will have different vector representations.

Sentence transformers apply pooling strategies (e.g., mean pooling) over token embeddings to produce a single vector per sentence.

\section*{Dimensionality and Use}
In this project, the model \texttt{MiniLM-L6-v2} produces 384-dimensional embeddings:
\[ \vec{v} \in \mathbb{R}^{384} \]
These embeddings are used to:
\begin{itemize}
  \item Represent books (title + description + author)
  \item Represent user queries
  \item Enable similarity search via FAISS
\end{itemize}

\section*{Similarity Metrics}
Cosine similarity and L2 (Euclidean) distance are common metrics. In this system, FAISS uses L2 distance:
\[ \text{dist}(\vec{q}, \vec{b}) = \| \vec{q} - \vec{b} \|_2^2 \]

Smaller distances imply stronger semantic similarity.

\section*{Benefits and Limitations}
\begin{itemize}
  \item \textbf{Pros:} Captures meaning beyond exact words; enables cross-domain generalization; fast inference with MiniLM.
  \item \textbf{Cons:} Embedding quality depends on input quality; short or vague inputs yield weak vectors.
\end{itemize}
