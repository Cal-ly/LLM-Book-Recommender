\chapter{Machine Learning Theory}
\label{appendix:theory}

\section{Natural Language Processing and Sentence Embeddings}
\label{sec:theory-nlp}

\subsection*{From Words to Vectors}

Computers do not understand text in its raw form. To work with natural language, we convert words and sentences into numbers â€” specifically, into vectors. A vector is a list of numbers that represents some underlying properties or meaning of a word or sentence.

Early methods like Word2Vec or GloVe learned vectors for each word based on the company it keeps (i.e., word context). For example, vectors for \texttt{king}, \texttt{queen}, and \texttt{man} followed patterns like:

\[
\texttt{king} - \texttt{man} + \texttt{woman} \approx \texttt{queen}
\]

However, word vectors don't consider the meaning of full sentences. That's where \textit{sentence embeddings} come in.

\subsection*{Sentence Embeddings}

Sentence embeddings turn an entire sentence or paragraph into a fixed-size vector that captures its meaning. The model used in this project is \texttt{all-MiniLM-L6-v2}, which outputs 384-dimensional vectors.

\[
\vec{v} = f_{\text{MiniLM}}(t), \quad \vec{v} \in \mathbb{R}^{384}
\]

These vectors allow us to compare different pieces of text based on their semantic similarity.

\begin{figure}[H]
    \centering
    \begin{tikzpicture}[
      node distance=1.8cm and 1cm,
      every node/.style={align=center, font=\small},
      process/.style={rectangle, draw, fill=blue!10, rounded corners, minimum width=2.8cm, minimum height=1cm}
    ]
    
    \node (text) [process] {Book Description\\(Natural Language)};
    \node (model) [process, right=of text] {MiniLM\\Sentence Transformer};
    \node (vector) [process, right=of model] {Embedding\\$\vec{v} \in \mathbb{R}^{384}$};
    
    \draw[->, thick] (text) -- (model);
    \draw[->, thick] (model) -- (vector);
    
    \end{tikzpicture}
    \caption{Generating sentence embeddings from raw text}
    \label{fig:embedding-pipeline}
\end{figure}

---

\section{Transformers and Self-Attention}
\label{sec:theory-transformers}

Transformers are a breakthrough architecture in NLP. Instead of reading text one word at a time (like RNNs), they use a mechanism called \textit{self-attention} to process all words simultaneously and understand how each word relates to the others.

\subsection*{Intuition: The Importance of Words}

Consider the sentence:
\begin{quote}
    \texttt{"The book she gave me was amazing."}
\end{quote}
The word \texttt{"she"} refers to a person mentioned earlier, and \texttt{"gave"} connects to both \texttt{"book"} and \texttt{"me"}. A transformer learns these relationships using self-attention.

\begin{figure}[H]
    \centering
    \begin{tikzpicture}[
        every node/.style={font=\small},
        word/.style={draw, rounded corners, fill=blue!10, minimum width=1.6cm, minimum height=0.9cm},
        attn/.style={->, thick, draw=gray!70},
        node distance=0.8cm
    ]
    
    % Input words
    \node (the)   [word]                    {The};
    \node (book)  [word, right=of the]     {book};
    \node (she)   [word, right=of book]    {she};
    \node (gave)  [word, right=of she]     {gave};
    \node (me)    [word, right=of gave]    {me};
    
    % Arrows showing attention from "she"
    \draw[attn,->] (she) to[bend left=45] (the);
    \draw[attn,->] (she) to[bend left=30] (book);
    \draw[attn,->] (she) to[bend left=15] (gave);
    \draw[attn,->] (she) to[bend left=10] (me);
    
    % Highlighting attended word
    \draw[attn,->, red, thick] (she) -- (book);
    
    % Label
    \node[below=1.1cm of book, text width=5cm, align=center] {\textbf{Example:} \texttt{"She"} attends strongly to \texttt{"book"} and \texttt{"gave"}};
    
    \end{tikzpicture}
    \caption{Simplified self-attention from the word \texttt{"she"} to other words}
    \label{fig:self-attention}
\end{figure}
    
\subsection*{MiniLM Model}

MiniLM is a compressed, fast version of a larger model (BERT). It retains the ability to understand sentence-level semantics while being efficient enough to run locally. It outputs a single vector for each input sentence by aggregating hidden states via mean pooling.

---

\section{Vector Similarity and Distance Metrics}
\label{sec:theory-vectorspace}

\subsection*{High-Dimensional Space}

Each sentence is embedded into a 384-dimensional vector space. Although we cant visualize this directly, we can think of it as placing each sentence as a point in a "cloud" of meaning. The closer two points are, the more semantically similar they are.

\subsection*{L2 (Euclidean) Distance}

To find the most similar book to a user query, we measure the distance between vectors. The most common distance metric is the L2 (Euclidean) distance:

\[
\text{dist}(\vec{q}, \vec{b}_i) = \|\vec{q} - \vec{b}_i\|_2^2 = \sum_{j=1}^{384} (q_j - b_{i,j})^2
\]

Where:
\begin{itemize}
    \item $\vec{q}$ is the query embedding.
    \item $\vec{b}_i$ is the embedding of the $i$th book.
\end{itemize}

Alternatively, cosine similarity can be used, which compares the angle between vectors rather than their absolute distance, see \autoref{fig:vector-distance}.

\begin{figure}[H]
    \centering
    \begin{tikzpicture}[scale=1.4]
      % Axes
      \draw[->] (-0.5, 0) -- (3.5, 0) node[right] {$x$};
      \draw[->] (0, -0.5) -- (0, 3.5) node[above] {$y$};
    
      % Vectors
      \draw[->, thick, blue] (0, 0) -- (2.5, 1) node[midway, above] {$\vec{q}$};
      \draw[->, thick, red] (0, 0) -- (2, 2.5) node[midway, left] {$\vec{b}_i$};
    
      % Dashed line for distance
      \draw[dashed] (2.5,1) -- (2,2.5);
      \node at (2.5,2.1) {$\|\vec{q} - \vec{b}_i\|$};
    
      % Origin
      \node[below left] at (0,0) {0};
    
    \end{tikzpicture}
    \caption{Semantic similarity as distance between vectors}
    \label{fig:vector-distance}
\end{figure}

---

\section{Nearest Neighbor Search with FAISS}
\label{sec:theory-faiss}

Searching for the closest vectors becomes computationally expensive as the dataset grows. FAISS (Facebook AI Similarity Search) is an open-source library for efficient nearest neighbor search in high-dimensional spaces.

\subsection*{IndexFlatL2}

In this project, we use \texttt{IndexFlatL2}, which performs exact nearest-neighbor search based on L2 distance. While not the most scalable option for millions of vectors, it is fast and accurate for a few thousand entries.

FAISS indexes the vectors and allows quick lookup of the top-$k$ nearest neighbors to a given query vector.

---

\section{Recommender Systems: Content-Based Filtering}
\label{sec:theory-recommenders}

\subsection*{Two Main Types}

There are two main types of recommender systems:
\begin{itemize}
    \item \textbf{Collaborative filtering:} Based on user behavior and preferences.
    \item \textbf{Content-based filtering:} Based on item attributes (e.g., descriptions, genres).
\end{itemize}

\begin{figure}[H]
    \centering
    \begin{tikzpicture}[node distance=1.4cm and 1.6cm,
      process/.style={rectangle, draw, rounded corners, fill=blue!10, minimum width=3.3cm, minimum height=1cm},
      data/.style={rectangle, draw, fill=gray!10, minimum width=3cm, minimum height=1cm},
      every node/.style={align=center, font=\small}
    ]
    
    % Content-based
    \node (desc1) [data] {Book Description};
    \node (cb1) [process, below=of desc1] {Content-based Model};
    \node (rec1) [data, below=of cb1] {Recommended Books};
    
    % Collaborative
    \node (hist1) [data, right=4.2cm of desc1] {User Ratings};
    \node (cb2) [process, below=of hist1] {Collaborative Filter};
    \node (rec2) [data, below=of cb2] {Recommended Books};
    
    % Labels
    \node[above of=desc1, font=\bfseries, node distance=0.8cm] {Content-Based Filtering};
    \node[above of=hist1, font=\bfseries, node distance=0.8cm] {Collaborative Filtering};
    
    % Arrows
    \draw[->, thick] (desc1) -- (cb1);
    \draw[->, thick] (cb1) -- (rec1);
    \draw[->, thick] (hist1) -- (cb2);
    \draw[->, thick] (cb2) -- (rec2);
    
    \end{tikzpicture}
    \caption{Filtering approaches: content-based vs. collaborative}
    \label{fig:filtering-comparison}
\end{figure}

\subsection*{Why Content-Based?}

In this project, we use content-based filtering because:

\begin{itemize}
    \item No user data is collected (privacy-by-design).
    \item Recommendations are based on semantic similarity between descriptions.
    \item The system runs fully offline.
\end{itemize}

This approach allows users to describe what they want in natural language and receive books that match the idea, not just a specific keyword.

---

\section{Local ML vs. Cloud ML}
\label{sec:theory-localcloud}

Running ML models locally has several trade-offs:

\subsection*{Advantages}
\begin{itemize}
    \item \textbf{Privacy:} No data leaves the user's device.
    \item \textbf{Offline access:} The system works without internet.
    \item \textbf{Cost:} No cloud infrastructure needed.
\end{itemize}

\subsection*{Limitations}
\begin{itemize}
    \item Limited personalization or learning over time.
    \item More constrained by local hardware.
    \item Cannot use large models like GPT-4 or full BERT with ease.
\end{itemize}

Still, models like MiniLM make local semantic search practical and useful in real-world applications.
    