\chapter{Evaluation and Performance Metrics}
\label{appendix:evaluation}

This appendix describes the evaluation strategy used to assess the performance of the local book recommendation system. Since no ground-truth labels or user feedback data were available, traditional supervised metrics (e.g., accuracy, precision, recall) could not be applied. Instead, alternative metrics were used to evaluate semantic relevance, system responsiveness, and overall usability.

\section{Constraints on Evaluation}
\begin{itemize}
    \item \textbf{No user history:} No personalized or collaborative filtering baseline.
    \item \textbf{No labeled test set:} No “correct” genre or match annotations.
    \item \textbf{Offline-only:} All testing conducted without server-side analytics or live monitoring.
\end{itemize}

This necessitated a human-in-the-loop, qualitative approach.

\section{Evaluation Criteria}
Four core criteria guided system evaluation:

\subsection*{1. Semantic Relevance}
Assessed whether the top-\( k \) results for a query were semantically aligned with its intent. This was evaluated manually using test queries such as:

\begin{quote}
\textit{“Books about surviving in space”} → Check that returned books involve space travel, isolation, or survival themes.
\end{quote}

In most test cases, at least 4 of the top 6 results were contextually appropriate.

\subsection*{2. Filtering Utility}
Tested whether genre and rating filters improved or restricted result precision. Filters were toggled and queries rerun to compare:

\begin{itemize}
    \item Breadth of result diversity
    \item Preservation of relevance post-filter
\end{itemize}

This highlighted that over-filtering may remove semantically correct results.

\subsection*{3. Responsiveness}
Measured latency of key operations on CPU-only hardware:

\begin{itemize}
    \item Query embedding: ~150ms
    \item FAISS search (top 60): <50ms
    \item Filtering and rendering: <300ms
\end{itemize}

Sub-second feedback ensured smooth interactivity.

\subsection*{4. Offline Functionality}
Validated that all components (embedding, classification, filtering, UI) function without network access. This included running:

\begin{itemize}
    \item Zero-shot classification with cached local models
    \item Full semantic search using only local vector index
    \item No calls to online services or third-party APIs
\end{itemize}

\section{Example Query Types}
Different categories of queries were tested:
\begin{itemize}
    \item \textbf{Thematic:} “Books about AI ethics”
    \item \textbf{Mood-driven:} “Something dark and mysterious”
    \item \textbf{Specific:} “Books like Dune”
\end{itemize}

Semantic embedding enabled flexible and expressive querying even when keywords were absent.

\section{Limitations of Evaluation}
\begin{itemize}
    \item No user feedback loop to refine metrics or iterate design
    \item No A/B testing against alternatives (e.g., TF-IDF or keyword search)
    \item Evaluation is largely anecdotal and lacks quantitative validation
\end{itemize}

Nevertheless, the qualitative results were consistent and satisfactory for the project scope.

\section{Conclusion}
Evaluation in low-data, offline-first ML projects requires adapted criteria. By focusing on semantic quality, system speed, and functional constraints, meaningful insights were still obtained. 
The framework can be extended in future work with automated tests or user trials.

