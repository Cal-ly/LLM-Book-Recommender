\chapter{Methodology and Structure}
\label{chapter:methodology}

This project follows a research-based prototype methodology. 
Rather than developing a commercial software product, the goal is to explore the scientific and 
technical feasibility of running semantic book recommendations using local machine learning tools.
The findings—both theoretical and experimental—constitute the core deliverables of this project.

\section{Research Approach}
\label{sec:research-approach}

The project is based on a combination of literature review and hands-on implementation. 
Each aspect of the methodology was selected to support answering the sub-questions defined in Section~\ref{sec:problem-definition}.

\begin{itemize}
    \item \textbf{Literature Review:} Relevant topics included natural language processing (NLP), sentence embedding techniques, 
    recommender systems, and similarity search. Key technologies such as \texttt{MiniLM}, \texttt{FAISS}, 
    and \texttt{Streamlit} were studied through documentation and academic sources.
    
    \item \textbf{Implementation:} The following tools and libraries were used throughout the project:
    \begin{itemize}
        \item \texttt{Python} for scripting, preprocessing, and experimentation.
        \item \texttt{pandas}, \texttt{seaborn}, and \texttt{matplotlib} for data analysis and visualization.
        \item \texttt{sentence-transformers} (MiniLM-L6-v2) to generate semantic vector representations.
        \item \texttt{FAISS} to index and search high-dimensional embeddings efficiently.
        \item \texttt{Streamlit} to build an interactive, privacy-preserving user interface.
    \end{itemize}
    
    \item \textbf{Testing:} Practical testing included a range of user queries and filtering conditions to observe whether recommendations matched the query intent semantically.
\end{itemize}

\section{Evaluation Criteria}
\label{sec:evaluation-criteria}

As no supervised training or labeled ground-truth data was involved, the system is evaluated using non-traditional metrics. The following criteria were applied:

\begin{itemize}
    \item \textbf{Qualitative Relevance:} Whether the recommendations appear semantically relevant to a human evaluator.
    \item \textbf{Responsiveness:} How quickly the system responds to user queries on consumer hardware.
    \item \textbf{Offline Capability:} Verification that all processing occurs locally, without internet access.
    \item \textbf{Scalability:} Exploration of performance with larger datasets and indexing sizes.
\end{itemize}

This methodology supports the central research question posed in \autoref{itm:main-question}, and particularly sub-questions~\ref{itm:subq-embedding} and~\ref{itm:subq-similarity}, by grounding each system component in both theoretical research and empirical testing.
