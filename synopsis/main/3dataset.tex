\chapter{Dataset Exploration}
\label{chapter:dataset}

The project began with the public dataset by Castillo (2025), containing metadata for over 6,800 books. Initial inspection revealed missing or inconsistent fields such as authorship, categories, and descriptions. A modular preprocessing pipeline was built to clean, augment, and prepare the data for semantic modeling.

\section{Dataset Overview}
The original dataset (\texttt{books.csv}) included fields such as:
\begin{itemize}
\item \verb|isbn13|, \verb|title|, \verb|subtitle|, \verb|authors|
\item \verb|categories|, \verb|description|, \verb|published_year|
\item \verb|average_rating|, \verb|num_pages|
\end{itemize}
Key fields were combined and engineered into a unified \verb|full_title|, and several boolean \verb|has_*| flags were created for inspection and filtering.

\section{Data Cleaning and Augmentation}
Cleaning and augmentation were performed in multiple stages:
\begin{enumerate}
\item \textbf{Initial Checks:} Detected and logged missing or invalid entries.
\item \textbf{OpenLibrary Augmentation:} Filled in missing values such as \texttt{authors}, \texttt{num\_pages}, and \texttt{thumbnail}. Introduced \texttt{subjects}.
\item \textbf{Google Books API:} Prioritized short or missing descriptions and added alternate fields (e.g., \texttt{description\_google}).
\item \textbf{Field Comparison:} Logged mismatches in fields like title and author between sources. Created \texttt{alt\_*} fields where minor but significant discrepancies were found.
\item \textbf{Final Merging:} Consolidated \texttt{categories}, \texttt{subjects}, and \texttt{categories\_google} into a cleaned \texttt{final\_categories} field.
\end{enumerate}
Records with less than 9 words in the final description were removed, reducing the dataset from 6,810 to 6,572 entries.

\section{Feature Engineering}
New fields were engineered to support classification and filtering:
\begin{itemize}
\item \verb|words_in_description| \textemdash{} token count of description
\item \verb|description_length| \textemdash{} character count of enriched description
\item \verb|has_*| flags \textemdash{} completeness indicators for filtering
\end{itemize}
These were used both for data readiness assessments and visualization.

\section{Exploratory Analysis}
Visual and statistical analysis was used to inform thresholds and highlight data issues:
\begin{itemize}
\item \textbf{Missing Values:} \verb|openlib_values_heatmap.png|
\item \textbf{Rating Distribution:} \verb|rating_distribution.png|
\item \textbf{Publication Year:} \verb|publication_year_distribution.png|
\item \textbf{Category Frequency:} \verb|top_categories.png|
\item \textbf{Short Descriptions:} \verb|less_than_50_words_description.png|
\item \textbf{Metadata Conflicts:} \verb|reexp_mismatch_counts.png|
\end{itemize}
These figures helped guide filtering strategies and offered insight into metadata quality.

\section{Outcome}
After augmentation and filtering, 6,572 books remained. These were passed to the category inference pipeline (Chapter~\ref{chapter:categories}), where further refinement reduced the set to 5,160 high-confidence entries suitable for semantic embedding and indexing.