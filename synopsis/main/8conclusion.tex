\chapter{Conclusion}
\label{chapter:conclusion}

\section{Discussion}
\label{sec:discussion}

The system successfully demonstrates that modern NLP models can be used to recommend books based purely on their textual content, without any need for collaborative data or internet access.

---

\section{Conclusion}
\label{sec:conclusion}

The following sub-questions were addressed:

\subsection*{1. What techniques exist for embedding text into meaningful vectors?}
Pretrained transformer-based models like \texttt{MiniLM-L6-v2} provide high-quality semantic embeddings. These models output fixed-size vectors that encode context-aware meaning, allowing comparisons based on semantic similarity rather than simple keyword overlap.

\subsection*{2. How can vector similarity be used for finding similar books?}
The project utilized L2 distance between embedding vectors to measure similarity. FAISS enabled fast nearest-neighbor retrieval, making it possible to find semantically similar books efficiently even on consumer-grade hardware.

\subsection*{3. What are the limitations of a local, content-only recommender?}
The system lacks user personalization and cannot learn from implicit user behavior (e.g., clicks or favorites). Recommendations are purely content-based and may be less diverse. The system also relies heavily on the quality and length of descriptions. However, these trade-offs are acceptable given the privacy and portability benefits of a local-only solution.

\textbf{Main research question:}

\begin{quote}
\textit{How can a local ML model be used to recommend books based on natural language descriptions, relying only on locally running models?}
\end{quote}

This synopsis has shown that a local recommender system based on sentence embeddings and vector similarity search is not only feasible but also effective. By leveraging compact models like MiniLM and indexing with FAISS, users can interact with an intelligent recommendation engine completely offline.

The findings support the broader hypothesis that smaller LLMs and sentence encoders can power local-first AI systems with practical value, while maintaining user privacy.

---

\section{Reflection}
\label{sec:reflection}

Throughout the project, several valuable insights were gained:

\begin{itemize}
    \item \textbf{Text preprocessing and quality matter:} The richness of the book descriptions significantly affected the semantic performance of the model.
    \item \textbf{Model performance exceeded expectations:} MiniLM delivered strong semantic matching capabilities despite its small size and fast inference time.
    \item \textbf{Local-first ML is viable:} Embeddings, indexing, and filtering can all be performed within a local Python environment using open-source libraries.
    \item \textbf{Trade-offs exist:} While privacy is enhanced, the lack of personalization and training capabilities limits the scope compared to cloud-based systems.
\end{itemize}

With more time, additional experiments could be conducted to:
\begin{itemize}
    \item Compare multiple embedding models (e.g., MPNet, BERT).
    \item Combine semantic search with metadata-based re-ranking.
    \item Evaluate using human-labeled query relevance.
\end{itemize}

Overall, the project provided hands-on experience with modern NLP, vector-based search, and interactive ML applications, combined in a working application.

