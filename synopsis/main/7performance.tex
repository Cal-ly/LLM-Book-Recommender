\chapter{Performance and Evaluation Challenges}
\label{chapter:performance}

This chapter discusses performance evaluation and the challenges involved in applying traditional machine learning metrics to a content-based recommendation system with no user interaction or ground-truth labels.

\section{Evaluation Without Labels}
\label{sec:no-labels}

Unlike supervised learning, the book recommender system does not predict a known output (e.g., a genre, rating, or success score). Instead, it performs semantic search in an embedding space. As a result:

\begin{itemize}
    \item There is no “correct answer” for what books should be recommended for a given query.
    \item Standard classification metrics such as accuracy, precision, recall, or F1-score are not applicable.
    \item User satisfaction, which is often used to measure recommender performance, cannot be captured in an offline and non-interactive system.
\end{itemize}

\section{Qualitative Evaluation}
\label{sec:qualitative-eval}

The primary evaluation method was qualitative:

\begin{itemize}
    \item Manually testing a variety of queries to check whether the recommended books aligned with the query intent.
    \item Verifying that books returned for vague or abstract queries (e.g., “a dystopian society run by algorithms”) still showed thematic coherence.
    \item Testing edge cases such as short queries, factual queries, or genre-specific phrases.
\end{itemize}

This form of evaluation relies on the developer's or test user's intuition and expectations, and while subjective, it can still provide meaningful insight into whether the system behaves as expected.

\section{Responsiveness and Latency}
\label{sec:latency}

Quantitative measurements were more relevant in evaluating system responsiveness:

\begin{itemize}
    \item \textbf{Embedding latency:} MiniLM embedding a single query took < 0.2 seconds on CPU.
    \item \textbf{Similarity search latency:} FAISS returned top-50 neighbors in < 10ms using \texttt{IndexFlatL2}.
    \item \textbf{UI response time:} Streamlit rendered results with image loading in under 2 seconds.
\end{itemize}

All performance metrics were recorded on standard consumer hardware (no GPU), confirming the system's suitability for local deployment.

\section{Scalability Considerations}
\label{sec:scaling}

While performance was satisfactory for ~6,500 books, scalability is still a concern:

\begin{itemize}
    \item FAISS's exact search is fast for thousands of vectors but may require approximate indexing (e.g., HNSW) for millions.
    \item The embedding model is fixed. If re-embedding is needed (e.g., after model upgrade), this requires full recomputation.
    \item Filtering and sorting are done in Python post-query, which may need optimization in larger datasets.
\end{itemize}

\section{Limitations of Offline Evaluation}
\label{sec:limitations}

Without user feedback or A/B testing:

\begin{itemize}
    \item Personal relevance of recommendations is unknown.
    \item Discovery potential (novel but interesting books) cannot be measured.
    \item Biases from the dataset (e.g., popular authors, dominant genres) may go unnoticed.
\end{itemize}

\section{Summary}
\label{sec:performance-summary}

Evaluating content-based recommenders in a local, non-interactive setting requires a shift from classical ML metrics to a combination of:

\begin{itemize}
    \item Qualitative inspection
    \item Usability testing
    \item System performance (latency)
\end{itemize}

If this project were to move further, a future extensions could include a small user study or feedback mechanism to gather more empirical evidence of recommendation quality.
