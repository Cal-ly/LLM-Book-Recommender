\chapter{Category Inference}
\label{chapter:category-inference}

To support genre-aware filtering and embedding, each book in the dataset is automatically labeled with one or more thematic categories. 
This process combines transformer-based zero-shot classification with a curated keyword-based fallback strategy, confidence-based filtering, and semantic cleaning. The refined categories are ultimately used for filtering, indexing, and interface interaction.

\section{Zero-Shot Classification with BART-MNLI}
Each book is first enriched with an \texttt{augmented\_description} field, combining its full title, author, publication year, and description. 
These are then fed into a zero-shot classification model, \texttt{facebook/bart-large-mnli} \cite{bart-large-mnli}, using Hugging Face’s \texttt{pipeline} for multi-label inference across 13 candidate categories (e.g. \textit{Fantasy}, \textit{Love}, \textit{Non-fiction}).

Predictions below a confidence threshold of 0.4 are discarded. Each book retains only the labels with strong relevance, based on model probabilities.

\section{Fallback Keyword Enrichment}
To improve recall and thematic depth, fallback keywords are used when the model fails to detect relevant categories. 
Each category has a corresponding keyword list (e.g. \texttt{“magic”}, \texttt{“orc”}, \texttt{“griffin”} for \textit{Fantasy}; \texttt{“vampire”}, \texttt{“haunted”}, \texttt{“exorcism”} for \textit{Horror}).

Fallback labels are only added if the model’s predictions do not already include them. In total, fallback logic affected 4665 out of 6572 rows.

\section{Refinement, Merging, and Metrics}
To reduce redundancy and overlap, closely related categories are merged:
\begin{itemize}
  \item \textbf{Biography + History} $\rightarrow$ \textit{Historical}
  \item \textbf{Philosophy + Poetry} $\rightarrow$ \textit{Philosophy \& Poetry}
  \item \textbf{Suspense + Detective} $\rightarrow$ \textit{Mystery}
\end{itemize}

Per-category precision, recall, and F1 scores are calculated using known metadata labels for validation. Visualizations and metric CSVs are generated for review.

\section{Confidence Analysis and Filtering}
Each row is evaluated using the following confidence metrics:
\begin{itemize}
  \item \texttt{max\_score}: Highest label confidence
  \item \texttt{filtered\_avg\_score}: Average score excluding labels under 0.2
  \item \texttt{score\_std}: Standard deviation across all label scores
  \item \texttt{num\_categories}: Total confident categories retained
\end{itemize}

To prepare for indexing, only rows satisfying all of the following are retained:
\begin{itemize}
  \item Description length $\geq$ 200 characters
  \item \texttt{filtered\_avg\_score} $\geq$ 0.2
  \item \texttt{max\_score} $\geq$ 0.4
  \item At least one confident category
\end{itemize}

This reduced the dataset from 6572 to 5160 books.

\section{Length vs Confidence Correlation}
To verify the impact of text richness, correlation between \texttt{description\_length} and \texttt{average\_score} was analyzed. The results:

\begin{itemize}
  \item Pearson correlation: $r=0.398$ ($p<0.0001$)
  \item Spearman rank: $\rho=0.261$ ($p<0.0001$)
\end{itemize}

This indicates a moderate positive relationship between description length and model confidence, validating the filtering step as both qualitative and statistically grounded.
