\chapter{Vector Similarity Search with FAISS}
\label{chapter:similarity}

Once book and query texts are embedded into dense vector space, the task becomes identifying which book vectors are most similar to a given query vector. This is accomplished using FAISS — Facebook AI Similarity Search \cite{faiss}, a library optimized for fast vector comparisons.

\section{Similarity Search Theory}
\label{sec:similarity-theory}

Given a user query $q$ and a set of embedded book vectors $\{\vec{b}_1, \vec{b}_2, \dots, \vec{b}_n\}$, the system computes similarity using L2 distance:

\[
\text{dist}(\vec{q}, \vec{b}_i) = \|\vec{q} - \vec{b}_i\|_2^2
\]

Where:
\begin{itemize}
    \item $\vec{q}$ is the embedding of the user query
    \item $\vec{b}_i$ is the embedding of the $i$th book entry
    \item The top $k$ results with the smallest distances are returned
\end{itemize}

This assumes semantically similar inputs are located close together in vector space — an assumption supported by transformer-based embeddings.

\section{Why FAISS?}
\label{sec:faiss-choice}

FAISS is designed for efficient similarity search across high-dimensional data. It is widely used in recommendation, retrieval, and clustering tasks.

Key advantages include:

\begin{itemize}
    \item \textbf{Speed:} Real-time query responses even with thousands of entries.
    \item \textbf{Simplicity:} Flat indexes (e.g. \texttt{IndexFlatL2}) support exact search.
    \item \textbf{Offline Support:} FAISS runs locally on CPU, making it ideal for private, embedded deployments.
\end{itemize}

In this project, we use \texttt{IndexFlatL2} for exact nearest-neighbor search using Euclidean distance.

\section{Implementation}
\label{sec:similarity-implementation}

After embedding all book entries using \texttt{MiniLM-L6-v2}, the following FAISS-based steps are applied:

\begin{enumerate}
    \item A FAISS index is created with \texttt{faiss.IndexFlatL2(384)}.
    \item All 5,160 book vectors are added to the index.
    \item The index is saved as \texttt{embeddings/index.faiss}.
    \item At runtime:
    \begin{itemize}
        \item A user query is embedded using the same transformer model.
        \item The top $k$ closest vectors are retrieved via FAISS.
        \item Metadata is joined from \texttt{books\_indexed\_with\_embeddings.csv}.
    \end{itemize}
\end{enumerate}

This provides fast and accurate lookup for semantically similar books without relying on collaborative signals.

\section{Summary}
\label{sec:similarity-summary}

FAISS enables efficient real-time recommendations by using a dense vector index. Together with MiniLM-based embeddings, it allows natural language queries to surface relevant books using only their content. The result is a performant, local, and privacy-preserving recommendation engine.
