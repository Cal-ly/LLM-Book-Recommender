\chapter{Text Embedding}
\label{chapter:embedding}

The core of the recommendation system lies in transforming books and queries into a shared semantic space. 
This is accomplished by generating sentence embeddings that encode the meaning of input text into numerical vectors. Once embedded, semantic similarity can be computed between queries and books.

\section{Sentence Embedding Theory}
\label{sec:embedding-theory}

Sentence embeddings are dense vector representations of variable-length text sequences. 
They enable comparison of textual content by mapping semantically similar sentences to nearby points in vector space. The process relies on transformer-based models.

Let $f(t)$ be the embedding function applied to a string $t$, producing a vector $\vec{v} \in \mathbb{R}^{384}$:
\begin{equation}
\vec{v} = f(t), \quad \vec{v} \in \mathbb{R}^{384}
\end{equation}

This project uses \texttt{all-MiniLM-L6-v2} from the Hugging Face \texttt{sentence-transformers} library \parencite{sentence-transformers}, which produces 384-dimensional embeddings. 
It offers an excellent balance between speed and semantic quality, making it ideal for offline inference on CPU hardware.

\section{Embedding Implementation}
\label{sec:embedding-implementation}

Text embeddings were generated using a composite string field \texttt{search\_text}, constructed for each book as follows:

\begin{verbatim}
Title: {full_title}. Author: {authors}. Description: {description}
\end{verbatim}

This format ensures that both metadata and narrative content contribute to the semantic embedding. The following steps were performed:
\begin{enumerate}
    \item Loaded the final dataset \texttt{books\_indexed.csv}.
    \item Encoded each \texttt{search\_text} entry using MiniLM.
    \item Saved the resulting matrix of shape $(n, 384)$ to memory.
    \item Added all vectors to a FAISS index (see Chapter~\ref{chapter:faiss}).
    \item Saved metadata with vectors to \texttt{books\_indexed\_with\_embeddings.csv}.
\end{enumerate}

\section{Embedding Configuration}
\label{sec:embedding-config}

The embedding stage used the following parameters:
\begin{itemize}
    \item \textbf{Model:} \texttt{all-MiniLM-L6-v2}
    \item \textbf{Library:} \texttt{sentence-transformers}
    \item \textbf{Embedding Dimension:} 384
    \item \textbf{Distance Metric:} L2 (Euclidean)
    \item \textbf{Backend:} CPU inference
    \item \textbf{Input Field:} \texttt{search\_text}
    \item \textbf{Entries Embedded:} 5,160 books
\end{itemize}

\section{Summary}
\label{sec:embedding-summary}

The embedding pipeline enabled semantic similarity computations between books and queries. Using a compact transformer model, each entry was encoded into a vector capturing its latent meaning. 
This allowed content-based querying without relying on metadata fields like genre, author popularity, or user behavior. The resulting vectors were stored in a FAISS index for efficient similarity search.
