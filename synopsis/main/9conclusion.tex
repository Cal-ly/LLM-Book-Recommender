\chapter{Conclusion}
\label{chapter:conclusion}

This chapter summarizes the findings and outcomes of the project, reflects on its limitations, and revisits the original research questions.

\section{Discussion}
\label{sec:discussion}

This project demonstrated that transformer-based models can power a local, content-only book recommendation system. Without requiring user profiles or collaborative filtering, the system delivers semantically meaningful recommendations based entirely on book metadata and free-text queries.

The end-to-end system consisted of:
\begin{itemize}
    \item Augmented and cleaned metadata for over 6,800 books
    \item Zero-shot genre inference using BART-MNLI with fallback keyword enrichment
    \item Semantic embeddings via MiniLM
    \item FAISS-based vector search
    \item A responsive, private UI implemented in Streamlit
\end{itemize}

With no reliance on internet access, the system runs entirely offline and respects user privacy. It is suitable for low-resource environments and educational use cases.

\section{Conclusion}
\label{sec:conclusion}

The system successfully addressed all four research sub-questions:

\begin{enumerate}
    \item \textbf{What techniques exist for embedding text into meaningful vectors?}  
    Pretrained sentence transformers like MiniLM encode text into high-dimensional semantic vectors, enabling rich similarity comparisons.

    \item \textbf{How can vector similarity be used for finding similar books?}  
    FAISS enables fast nearest-neighbor retrieval in vector space, allowing queries to return top-matching books based on content alone.

    \item \textbf{How can genre categories be inferred and used for filtering?}  
    BART-MNLI enables zero-shot classification of genre labels. Fallback keyword rules and confidence filtering improve label coverage and reliability.

    \item \textbf{What are the limitations of a local, content-only recommender?}  
    The system lacks personalization and behavior-driven refinement. However, it compensates with transparency, offline execution, and semantic precision.
\end{enumerate}

\noindent
\textbf{Main research question:}

\begin{quote}
\textit{How can a local ML model be used to recommend books based on natural language descriptions, relying only on locally running models?}
\end{quote}

This was answered by developing and validating a fully local recommendation system that performs semantic classification, embedding, indexing, and retrieval â€” all on-device.

\section{Reflection}
\label{sec:reflection}

The project offered valuable insights into building ML-powered applications under strict resource constraints. Key lessons include:

\begin{itemize}
    \item \textbf{Description quality drives results:} Well-written descriptions significantly enhance semantic accuracy.
    \item \textbf{Fallbacks expand coverage:} Keyword-based strategies improve classification when the model lacks confidence.
    \item \textbf{Local-first ML is viable:} Even compact transformer models provide robust performance on consumer hardware.
    \item \textbf{Simplicity over complexity:} Lightweight, modular design reduced failure points and increased maintainability.
\end{itemize}

With more time, the system could be extended to include:

\begin{itemize}
    \item Comparison of embedding models (e.g., MPNet, SBERT)
    \item Re-ranking of results based on metadata
    \item Interactive user feedback or offline learning
\end{itemize}

The project demonstrates that privacy-respecting, offline-first NLP systems are not only possible but practical. This aligns with emerging interest in edge-based AI and user-sovereign computing.
