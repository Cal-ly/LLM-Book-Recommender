\chapter{Performance and Evaluation Challenges}
\label{chapter:performance}

This chapter outlines how the system’s performance was evaluated, given the absence of user feedback and labeled test data. Traditional metrics such as accuracy or precision are not directly applicable to semantic similarity tasks without ground truth. Therefore, evaluation is divided into subjective, statistical, and system-level components.

\section{Evaluation Without Labels}
\label{sec:no-labels}

The system does not predict a fixed label or class. Instead, it embeds both queries and books in a shared vector space and retrieves nearest neighbors using semantic distance. As a result:

\begin{itemize}
    \item There is no single “correct” recommendation per query.
    \item Precision/recall-based metrics cannot be used reliably.
    \item Feedback mechanisms like click-through or engagement data are unavailable in an offline setting.
\end{itemize}

This necessitates alternative evaluation strategies.

\section{Qualitative Evaluation}
\label{sec:qualitative-eval}

Evaluation relied on exploratory queries and manual inspection of results. Test queries covered a range of genres, tones, and abstraction levels. Examples include:

\begin{itemize}
    \item ``Existential loneliness in space''
    \item ``Post-apocalyptic survival story''
    \item ``Books that explore grief through fantasy''
\end{itemize}

Results were judged based on alignment between the query and the content of the returned book descriptions. This human-in-the-loop approach confirmed that semantically aligned results were typically returned.

\section{Category Inference Evaluation}
\label{sec:category-eval}

Although the original dataset lacked consistent genre labels, the quality of inferred categories was assessed via:

\begin{itemize}
    \item Retaining classification confidence scores per label
    \item Filtering by a threshold of 0.4 (see Chapter~\ref{chapter:categories})
    \item Computing precision, recall, and F1 scores for a subset of trusted entries
    \item Visualizing match counts and fallback coverage
\end{itemize}

Certain categories (e.g., \textit{Fantasy}, \textit{Love}) were strongly predicted by the model, while others (e.g., \textit{Philosophy \& Poetry}) relied more heavily on fallback keyword enrichment.

\section{Responsiveness and Latency}
\label{sec:latency}

Performance was measured on a consumer-grade laptop (no GPU):

\begin{itemize}
    \item Query embedding time (MiniLM): $< 0.2$ seconds
    \item FAISS top-60 vector search: $< 10$ ms
    \item Streamlit UI rendering (6 cards per page): $< 2$ seconds including image fallback
\end{itemize}

This confirms that the system operates in real time on modest hardware, supporting its offline-first goal.

\section{Scalability Considerations}
\label{sec:scaling}

While the current dataset (5,160 entries) is manageable, scaling raises practical challenges:

\begin{itemize}
    \item \textbf{Indexing:} FAISS IndexFlatL2 is efficient but linear in search time; approximate methods may be needed for larger corpora.
    \item \textbf{Re-embedding:} Changing models or descriptions requires recomputing all embeddings.
    \item \textbf{Filtering:} UI-side filtering and sorting scale linearly with post-search result volume.
\end{itemize}

Future extensions should consider vector compression, batch processing, and caching.

\section{Limitations of Offline Evaluation}
\label{sec:limitations}

Offline evaluation limits the types of insights that can be gathered:

\begin{itemize}
    \item \textbf{Relevance:} Relies solely on developer judgment
    \item \textbf{Discovery:} Cannot measure novelty or serendipity
    \item \textbf{Bias Detection:} Overrepresentation of certain authors or genres may go unnoticed
\end{itemize}

These are acceptable trade-offs in the context of a local, user-respecting prototype.

\section{Summary}
\label{sec:performance-summary}

Evaluation focused on semantic quality, practical responsiveness, and offline usability. Despite the lack of supervised benchmarks, confidence filtering, manual validation, and runtime measurements demonstrated that the system is performant and aligned with its design goals.
