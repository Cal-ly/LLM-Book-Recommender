\begin{frame}{Backup: Concrete Examples}

\begin{columns}[T]
  \begin{column}{0.50\textwidth}
    \textbf{Example Query 1:}
    \begin{beamercolorbox}[sep=4pt,left]{block body}
     \textit{"Books about artificial intelligence and ethics"}
    \end{beamercolorbox}
    
    \textbf{Top Results:}
    \begin{itemize}
      \item  "The Alignment Problem" - Brian Christian
      \item  "Life 3.0" - Max Tegmark  
      \item  "Weapons of Math Destruction" - Cathy O'Neil
    \end{itemize}

    \vspace{0.1cm}
    \textbf{Example Query 2:}
    \begin{beamercolorbox}[sep=4pt,left]{block body}
     \textit{"mystery novels with unreliable narrators"}
    \end{beamercolorbox}
    
    \textbf{Top Results:}
    \begin{itemize}
      \item  "Gone Girl" - Gillian Flynn
      \item  "The Girl on the Train" - Paula Hawkins
      \item  "In the Woods" - Tana French
    \end{itemize}
  \end{column}

  \begin{column}{0.45\textwidth}
    \textbf{What This Demonstrates:}
    \begin{itemize}
      \item  \highlight{Semantic understanding} beyond keywords
      \item  \highlight{Abstract concept matching} (ethics, unreliable narrators)
      \item  \highlight{Cross-genre discovery} potential
    \end{itemize}

    \vspace{0.1cm}
    \textbf{Evaluation Challenges:}
    \begin{itemize}
      \item  No ground truth for "perfect" recommendations
      \item  Subjective nature of book preferences
      \item  \highlight{Solution:} Focus on semantic relevance rather than prediction accuracy
    \end{itemize}

    \vspace{0.1cm}
    \textbf{Discussion Starters:}
    \begin{itemize}
      \item  How could one evaluate recommendation quality
      \item  Books with sparse descriptions
      \item  Could this approach work for other domains?
    \end{itemize}
  \end{column}
\end{columns}

\end{frame}